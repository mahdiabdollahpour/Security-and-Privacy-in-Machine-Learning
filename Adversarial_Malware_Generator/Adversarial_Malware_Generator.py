import torch
import torch.nn.functional as F
import torch.nn as nn
from MalwareDetector import MalwareDetector
import numpy as np
import sys
import struct
import os
from tqdm import tqdm
import torch.optim as optim


def convert_adversarialEmbedding_to_closest_bytes(perturbation_embedding, byte_embedding):
    perturbation_len = perturbation_embedding.size()[0]
    byte_len = byte_embedding.size()[0]

    perturbation_bytes = torch.zeros(perturbation_len)


    for i in tqdm(range(perturbation_len)):
        distances = torch.zeros(256)

        # for each possible byte, compute the euclidean distance between the byte's embedding and the adversarial byte embedding ( to find the corresponding byte that represents this adversarial byte embedding)
        for j in range(byte_len):

            distances[j] = ((perturbation_embedding[i] - byte_embedding[j])**2).sum()
        # find the closest byte embedding that corresponds to the current adversarial byte in perturbation embedding
        # TODO
        idx = torch.argmin(distances)
        perturbation_bytes[i] = idx
    return perturbation_bytes


def fgsm_malware_generator():
    torch.manual_seed(0)
    np.random.seed(0)

    # Maximum number of bytes that is allowed to append at the end of malware file
    payload_size = 256

    # Window size of the malware detector
    window_size_detector = 512

    # epsilon value for FGSM attack on byte embedding ( note that this is used to find an embedding for an adversarial byte, so if its too large or too small we can't find a proper new embedding and attack will fail )
    epsilon = 0.7

    # label of benign files is 0, this is our target label
    benign_label = 0

    # maximum number of iterations for generating the adversarial malware
    max_loop = 20

    # threshold for detecting a file as a benign file
    benign_threshold = 0.95

    # Read the malware file
    # with open(sys.argv[1], 'rb') as f:
    with open('Malware_DoNotExecute.exe', 'rb') as f:
        malware_bytes = f.read()

    # Loading the malware detector model
    MalwareDetector = MalwareDetector(channels=256, window_size=window_size_detector, embd_size=8)
    weights = torch.load('./MalwareDetector.checkpoint', map_location='cpu')
    MalwareDetector.load_state_dict(weights['model_state_dict'])
    MalwareDetector.eval()

    # Set payload size to append to malware file
    end_of_file_offset = len(malware_bytes)
    perturbation_padding_size = window_size_detector - np.mod(end_of_file_offset + payload_size, window_size_detector)
    perturbation_padding_size = np.mod(perturbation_padding_size, window_size_detector)

    print("Size of original file : ", end_of_file_offset)
    print("Payload size : ", payload_size)
    print("Padding size : ", perturbation_padding_size)

    # Embed the malware file bytes, each byte is converted to a 8 dimension tensor
    embed = MalwareDetector.embd
    byte_embedding = embed(torch.arange(0, 256))

    # Set target label, here target label is the benign label since we are trying to fool the model to think this is a benign file
    target_label = torch.tensor([benign_label], dtype=torch.long)

    # Initial perturbation bytes
    perturbation_bytes = np.random.randint(1, 254, payload_size, dtype=np.uint8)
    perturbation_padding = np.zeros(perturbation_padding_size, dtype=np.uint8)

    # Conver malware bytes to a numpy array
    malware_npArray = np.frombuffer(malware_bytes, dtype=np.uint8)

    tmp = torch.tensor([benign_label], dtype=torch.long)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(sMalwareDetector.parameters(), lr=0.001, momentum=0.9)
    for i in range(max_loop):
        concat_args = (malware_npArray, perturbation_bytes, perturbation_padding)
        adv_malware_npArray = np.concatenate(concat_args)[np.newaxis, :]
        adv_malware_npArray = torch.from_numpy(adv_malware_npArray).float()
        embedded_adv_malware = embed(adv_malware_npArray.long()).detach()

        # Set requires_grad attribute of tensor.
        embedded_adv_malware.requires_grad = True

        # Give the malware to the model and receive the output
        model_output = MalwareDetector(embedded_adv_malware)

        results = F.softmax(model_output, dim=1)
        benign_prob = results[0][0].item()
        malware_prob = results[0][1].item()

        print('Benign probability : ', benign_prob, ', Malware probability : ', malware_prob, ', Current iteration : ',
              i)

        # Compute loss
        loss = criterion(results, torch.tensor([target_label]))

        print('Loss: ', loss.item())

        # Zero all existing gradients
        optimizer.zero_grad()

        # Back Propagation
        loss.backward()

        # Extract the grad sign of perturbation_bytes
        grad_sign = embedded_adv_malware.grad.data.sign()

        # perturbation_embedding = embedded_adv_malware[0][-2*payload_size:-payload_size]
        perturbation_embedding = embed(torch.from_numpy(perturbation_bytes).long())

        # Compute the new adversarial perturbation_embedding
        # TODO
        perturbation_embedding = perturbation_embedding - epsilon * grad_sign[0][-2*payload_size:-payload_size]



        # Change the appeneded bytes at the end of malware file to the new perturbation
        embedded_adv_malware = embedded_adv_malware.detach().numpy()
        embedded_adv_malware[0][-payload_size:] = perturbation_embedding.detach().numpy()

        # Convert embedded perturbation ( which in this embedding, each byte is a 8 dimension tesnor) bytes to real bytes
        # It will find closest possible bytes for each of the new adversarial embedded bytes
        perturbation_bytes = convert_adversarialEmbedding_to_closest_bytes(
            torch.from_numpy(perturbation_embedding.detach().numpy()), byte_embedding).detach().numpy()

        # Make a decision on evasion rates
        if benign_prob > benign_threshold:
            print('[*] Generate adversarial sample successfully bypassed the Malware detector model ')
            print('benign probabilty : ', benign_prob)
            print('malware probabilty : ', malware_prob, '\n')

            adversarial_file_name = 'AdversarialExample.bin'

            with open(adversarial_file_name, 'wb') as adversarial_file:

                concat_args = (malware_npArray, perturbation_bytes, perturbation_padding)
                final_adversarial_file = np.concatenate(concat_args)

                for cur_byte in final_adversarial_file:
                    adversarial_file.write(struct.pack('B', int(cur_byte)))

            print('Final adversarial malware file has been created :  ', adversarial_file_name)
            print('Good Job, bye!')

            return

    print('Adversarial Example is not found :(')


if __name__ == '__main__':
    fgsm_malware_generator()
